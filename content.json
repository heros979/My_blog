{"pages":[{"title":"about","text":"待完工..","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"诗集","text":"还在等待诗人的到来…","link":"/poems/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"深度学习","text":"回顾机器学习定义一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。 机器学习的核心 数据 模型分类 有监督学习 回归 线性回归 分类 SVM 无监督学习 聚类 主成分分析 半监督学习 增强学习(Reinforcement Learning)学习过程(监督学习) 损失函数(loss function) 优化方法 梯度下降深度学习是什么 wiki：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。 深度学习是机器学习的子集 深度学习和传统机器学习算法的异同数据方面 Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。 计算量方面深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。 耗时量级： 传统机器学习：秒、分钟、小时 深度学习：小时、天、周 输入特征方面机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。 深度学习算法人工神经网络(ANN)其中的一个节点：激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。常用激活函数： relu x if x &gt; 0 0 if x &lt;= 0 tanh 卷积神经网络(CNN)卷积在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同. CNN结构两条基本假设： 最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征 图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——平移不变性 关键：局部连接，权值共享，池化 CNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。 模型训练模型训练的过程可以认为是使损失函数最小化的过程 模型泛化与过拟合、欠拟合问题因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。 书籍推荐入门书籍：该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。 进阶书籍：该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。 主流深度学习框架 Tensorflow Pytorch Keras Paddlepaddle 总结 人工智能&gt;机器学习&gt;深度学习 需要更多的数据量和算力 ANN 输出是输入的复杂非线性函数 CNN 局部连接 权值共享 池化 模型训练 损失函数 梯度下降 注意过拟合与欠拟合","link":"/2020/03/20/deep-learning/"},{"title":"果壳中的经典统计","text":"果壳中的经典统计钟昊均 摘要&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。 统计方法的引入&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。 &emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。 &emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。 经典统计的局限性&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。 &emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。 &emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。 &emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。 结束语&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。","link":"/2020/01/16/zhj-2/"},{"title":"读朗道力学有感——以最小作用量原理为第一性原理的经典演绎","text":"读朗道力学有感——以最小作用量原理为第一性原理的经典演绎钟昊均 摘要&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。 拉格朗日分析&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。 &emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。 哈密顿分析&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。 &emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。 &emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。 结束语&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。","link":"/2020/01/16/zhj-1/"},{"title":"Third Poem","text":"HIDING&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Booming, booming bus. Rains dropped on the windows. Outside is the dusky sky With dark clouds hiding somewhere. Inside is a complacent heart Without being complimenting. Hiding the disappointment From a beautiful fragile mind, Where intellect should always stand. I show my courage to my cold little heart, which might suffer from wild heat of tricking, but would not go into hiding Any more.","link":"/2019/12/17/poem3/"},{"title":"机器学习概览","text":"机器学习是什么定义 wiki：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。 Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。 一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。 让我引用一张图片来说明：注意：这里是用有监督模型举例(后面会对这个名词进一步解释) 针对机器学习最重要的内容 数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。 模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。 数据的分类 结构化数据———储存在数据库中的数据 非结构化数据——-语音信号、图像图形、自然语言 机器学习能干什么 语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译) 人脸识别(微信、支付宝、宿舍门禁) 量化交易(预测股市) 房价预测 推荐系统(淘宝、京东、抖音) 医生/老中医 解微分方程、不定积分(见：AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab) 寻找淹没在背景噪声中的小信号(Higgs Boson Machine Learning Challenge、引力波信号、引力透镜参数预测) 好用的机器学习库以及书籍推荐scikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)书籍： 机器学习的分类按照模型的学习方式，我们可以分为如下几类： 有监督学习对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。 有监督学习的任务一般是回归或者分类问题： 回归：线性回归比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。 分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。 无监督学习对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做聚类。 KMEAN 半监督学习介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。 增强学习增强学习是一种有反馈的学习方式。 例子：贪吃蛇问题一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。 机器学习的一般步骤数据采集和标记 构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动） 预测房价：面积大小、房间数、地段、楼龄等；芝麻信用：海量的用户交易数据； 数据清洗 对数据中的单位进行统一 去掉重复数据、噪声和数据缺失 特征选择 从哪些特征对进行机器学习是有用的；人工设计或自动选择 模型选择 根据问题选择模型，聚类还是分类，回归还是分类 模型训练和测试 把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据) 模型性能评估与优化 训练时长，训练数据是否足够，是否能满足需要 模型使用 训练好的模型进行存储，以备下次使用 机器如何学习损失函数例如：用一维线性回归举例: $$ y = kx+b $$ x就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？ 也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做损失函数。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的损失函数，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得损失函数取得最小值。$$L_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$这里的m表示数据个数。 除此之外，对于二分类问题来说，常用的损失函数是二元交叉熵损失(Logistic损失): $$L_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}{i}-\\left(1-y{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]$$ 这里的 $\\hat{y}$ 代表预测y=1的概率。 梯度下降我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用梯度下降算法来更新参数。 这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。 我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。 代码实现本文使用scikit-learn实现一个线性回归模型举例： 12345from sklearn.linear_model import LinearRegression #从sklearn引入线性回归模型model=LinearRegression() #声明线性回归模型model.fit(X_train,Y_train) #用训练数据训练模型Y_pred=model.predict(X_test) #用模型对测试数据做预测print(Y_pred) #输出预测结果 常用的API有： API 解释 fit(X_train,Y_train) 训练模型 predict(X_test) 预测测试数据的结果 score(X_test,Y_test) 测试预测数据的score(例如正确率) 这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见官方文档 总结 1.获取数据 2.处理数据(80%的时间) 3.训练模型(20%的时间) 4.进行预测 5.观察结果，不满意则重复2.3.4.步，满意则保存模型","link":"/2019/12/14/ml-start/"},{"title":"Second Poem","text":"POUNDING&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Pounding, pounding heart, Why you pump so hard? Feeling a bunch of vessels Extending to my roots, Where blood poured After running across Wvery finger of my nerves. Who am I Besides a cluster of cells, Tramping in the sounds...","link":"/2019/12/14/poem2/"},{"title":"First Poem","text":"LOST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Chilled night, chilly light, Wind rolling sky, Cloud over head. Ashes tangled my hair, Mind lost from ear. Running from sunlight, The music ignite a fire, Lit a face in the dusky theater air; The dance stepped the melody Into my eyes gently. Tears shining In the reflected screen light, Heart flipping Over the woebegone rejoicing With the beauty of art. I've lost my blue From their bloody mouth. Taping up and down, Their tip of tongue fan a fame. When there is no longer a pure face But tech race on the screen, They stop looking into themselves. I've shut my soul from heat of lies, But lost in my chilling heart. Looking outside From the window of fear, Everyone seems to be tired. Walking over again Like nothing changed there, I want my feeling back.","link":"/2019/12/12/poem1/"},{"title":"Hexo+markdown开始博客写作","text":"目录 Hexo安装 Hexo使用 Markdown写作 Hexo安装引用Hexo官方文档： 什么是 Hexo?Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。 安装前提安装 Hexo 相当简单，只需要先安装下列应用程序即可：Node.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)Git 如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。 安装 Git Windows：下载并安装 git. Mac：使用 Homebrew, MacPorts 或者下载 安装程序。 Linux (Ubuntu, Debian)：sudo apt-get install git-core Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： Windows：通过 nvs（推荐）或者nvm 安装。 Mac：使用 Homebrew 或 MacPorts 安装。 Linux（DEB/RPM-based）：从 NodeSource 安装。 其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。 这里需要用到刚才安装的git，在git bush里进行如下操作： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 $ npm install -g hexo-cli 这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。 Hexo使用如果你已经有了一个建好的网站，并且已经上传到了github，可将网站代码克隆到本地。 在选定的文件夹下： 1$ git clone git@github.com:heros979/My_blog.git clone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。 如果你是第一次建立自己的网站，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入： 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 其中folder是你存放代码的文件夹名字 好了，我们的文件夹里有了很多新的文件,如下： 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 其中 _config.yml 是网站的配置信息，您可以在此配置大部分的参数。如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的： 我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post 1hexo new &lt;post-name&gt; 我们在写完post后在根目录输入： 123hexo cleanhexo ghexo d 即可将我们的网站部署到VPS服务器和我们的github上。 Markdown写作打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。 例子1： 123456*开辟鸿蒙，谁为情种？* #这是斜体**都只为风月情浓。** #这是加粗***奈何天，伤怀日，*** #这是斜体加粗&lt;font size=4&gt;寂寥时，试遣愚衷。&lt;/font&gt; #自定义字体大小~~因此上，演出这~~ #这是删除线&gt; 怀金悼玉的《红楼梦》。 # 这是引用 开辟鸿蒙，谁为情种？都只为风月情浓。奈何天，伤怀日，寂寥时，试遣愚衷。因此上，演出这 怀金悼玉的《红楼梦》。 例子2： 123456789101112131415161718192021222324## CALLED BACK #这是标题，几个#就是几级标题&lt;font face=&quot;微软雅黑&quot; size=6 color=#FF0000 &gt;Just lost when I was saved!&lt;/font&gt; #修改字体字号颜色&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=orange&gt;Just felt the world go by!&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; #修改背景色Just girt me for the onset with eternity,When breath blew back,And on the other sideI heard recede the disappointed tide!Therefore, as one returned, I feel,Odd secrets of the line to tell!Some sailor, skirting foreign shores,Some pale reporter from the awful doors Before the seal!Next time, to stay!Next time, the things to seeBy ear unheard,Unscrutinized by eye.Next time, to tarry,While the ages steal,—Slow tramp the centuries,And the cycles wheel. CALLED BACKJust lost when I was saved! Just felt the world go by! Just girt me for the onset with eternity, When breath blew back, And on the other side I heard recede the disappointed tide! Therefore, as one returned, I feel,Odd secrets of the line to tell!Some sailor, skirting foreign shores,Some pale reporter from the awful doorsBefore the seal! Next time, to stay!Next time, the things to seeBy ear unheard,Unscrutinized by eye. Next time, to tarry,While the ages steal,—Slow tramp the centuries,And the cycles wheel. 更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。 参见：Markdown快速入门，Markdown语法(字体,样式,公式,背景,图表等)","link":"/2019/12/11/writing/"},{"title":"DigitalOcean+Hexo+Nginx+Namecheap搭建个人博客","text":"强烈推荐Github学生包，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。 技术架构 VPS：DigitalOcean 域名注册：NameCheap 博客框架：Hexo 自动部署：githook 前期准备 学生，或者有一个.edu后缀的邮箱 可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡） $5用来激活账户 Github学生认证这一步网络上有大量教程，本文不再赘述。 参见： 注册Github并进行学生认证 Github教程 学生认证 注册Paypal亲测中国银行借记卡(有union标志)可用。 参见： 注册申请PayPal支付账户 PayPal注册绑卡使用教程 注册DigitalOcean这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。 参见： 在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码 从领取Github教育礼包到DigitalOcean购买服务器 申请一个服务器新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）： 价格选择最便宜的，这样可以用接近一年 重点来了！！！ 服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt： 下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改 最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。 本地配置hexo怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在下一篇博客会讲。 部署Hexo到远程VPS服务器putty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。 具体过程参见：简书-VPS部署Hexo网站 现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。 注册域名/域名解析从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：Namecheap域名如何绑定IP 等待几分钟后，就可以通过你的.me域名进入网站了！","link":"/2019/12/10/blog-init/"},{"title":"2019大数据算法赛总结","text":"之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。 概览报名费用：¥150（每队） 参加人数： 290队 最终排名： 62 最终得分（AUC）：0.733 使用模型：XGBOOST 数据预处理数据概况train.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下: 缺失数据我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。 粗略观察数据和label的关系我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图： 扔掉基本没有差别的列：[‘E3’,’E5’, ‘E8’, ‘E9’,’E19’,’E26’’E29’] C2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。 CTR（Click-Through-Rate）问题长尾效应CTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）： 即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。 分箱处理有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。 举个例子早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。 分箱方法虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。 经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现： 不平衡数据/Ensemble通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。 模型选择我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型 第一版模型无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710 第二版模型进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了 第三版模型细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720 第四版模型用DeepFM做相同的事情，重新调参，AUC：0.720 第五版模型细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721 第六版模型将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731 最终模型细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733","link":"/2019/12/09/summury-of-big-data-competition/"}],"tags":[{"name":"VPS","slug":"VPS","link":"/tags/VPS/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"hginx","slug":"hginx","link":"/tags/hginx/"},{"name":"网站","slug":"网站","link":"/tags/%E7%BD%91%E7%AB%99/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"poem","slug":"poem","link":"/tags/poem/"},{"name":"bigdata","slug":"bigdata","link":"/tags/bigdata/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"写作","slug":"写作","link":"/tags/%E5%86%99%E4%BD%9C/"},{"name":"读后感","slug":"读后感","link":"/tags/%E8%AF%BB%E5%90%8E%E6%84%9F/"}],"categories":[{"name":"博客","slug":"博客","link":"/categories/%E5%8D%9A%E5%AE%A2/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"诗集","slug":"诗集","link":"/categories/%E8%AF%97%E9%9B%86/"},{"name":"物理","slug":"物理","link":"/categories/%E7%89%A9%E7%90%86/"}]}